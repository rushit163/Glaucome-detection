{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm  # For loading pre-trained transformers\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset loading\n",
    "csv_file = \"/home/kirtimaan/Projects/archive/Glaucoma.csv\"\n",
    "image_dir = \"/home/kirtimaan/Projects/archive/ORIGA/ORIGA/Images\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df['Glaucoma'] = df['Glaucoma'].astype(int)\n",
    "\n",
    "# Custom dataset class\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.dataframe.iloc[idx, 4]  # Glaucoma label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Transform for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Glaucoma'])\n",
    "\n",
    "train_dataset = GlaucomaDataset(train_df, image_dir, transform)\n",
    "test_dataset = GlaucomaDataset(test_df, image_dir, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model Creation\n",
    "def create_model(model_name, num_classes):\n",
    "    model = timm.create_model(model_name, pretrained=True)\n",
    "    model.head = nn.Linear(model.head.in_features, num_classes)  # Modify for binary classification\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "# Model names\n",
    "models_to_compare = {\n",
    "    \"vit_base_patch16_224\": \"Vision Transformer\",\n",
    "    \"deit_base_patch16_224\": \"DEIT\",\n",
    "    \"nat_base\": \"Neighborhood Attention Transformer\"\n",
    "}\n",
    "\n",
    "# Training and comparing accuracy\n",
    "results = {}\n",
    "\n",
    "for model_name, model_label in models_to_compare.items():\n",
    "    print(f\"\\nTraining {model_label}...\")\n",
    "    model = create_model(model_name, num_classes=2)\n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, epochs=5)\n",
    "    results[model_label] = accuracy\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model_label, acc in results.items():\n",
    "    print(f\"{model_label}: {acc*100:.2f}% accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import ViTModel, ViTConfig, create_model\n",
    "\n",
    "# Add necessary imports for DEIT and NAT when applicable\n",
    "\n",
    "# Load dataset\n",
    "dataset_dir = '/Users/rushitpatel/Desktop/Datasets/Images'\n",
    "csv_file = '/Users/rushitpatel/Desktop/Datasets/glaucoma.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Add full path to the filenames\n",
    "df['Filename'] = df['Filename'].apply(lambda x: os.path.join(dataset_dir, x))\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='Filename',\n",
    "    y_col='Glaucoma',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='Filename',\n",
    "    y_col='Glaucoma',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define Vision Transformer (ViT) model\n",
    "def create_vit_model(input_shape=(224, 224, 3), num_classes=1):\n",
    "    vit_config = ViTConfig()\n",
    "    vit = ViTModel(vit_config)\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    vit_outputs = vit(inputs)\n",
    "    \n",
    "    x = layers.Flatten()(vit_outputs[0])  # Assuming this is how ViT model provides output\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Define DEIT model\n",
    "def create_deit_model(input_shape=(224, 224, 3), num_classes=1):\n",
    "    # You need to load DEIT model from the huggingface or timm library\n",
    "    pass\n",
    "\n",
    "# Define Neighborhood Attention Transformer model\n",
    "def create_nat_model(input_shape=(224, 224, 3), num_classes=1):\n",
    "    # You need to load NAT model from its corresponding library\n",
    "    pass\n",
    "\n",
    "# Compile and train each model, storing their results\n",
    "\n",
    "models = {\n",
    "    'ViT': create_vit_model(),\n",
    "    'DEIT': create_deit_model(),\n",
    "    'NAT': create_nat_model()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,  # Modify based on your needs\n",
    "        validation_data=test_generator\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(test_generator)\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model_name, accuracy in results.items():\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520 validated image filenames belonging to 2 classes.\n",
      "Found 130 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Create the ViT model\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m vit_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m, in \u001b[0;36mcreate_vit_model\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Preprocess input for Vision Transformer (ViT)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m vit_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Flatten and pass through Dense layers for classification\u001b[39;00m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(vit_outputs[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# ViT outputs a tuple, we take the hidden states\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:608\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    606\u001b[0m expected_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mpatch_embeddings\u001b[38;5;241m.\u001b[39mprojection\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[0;32m--> 608\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m \u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(expected_dtype)\n\u001b[1;32m    610\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    611\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    612\u001b[0m )\n\u001b[1;32m    614\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    615\u001b[0m     embedding_output,\n\u001b[1;32m    616\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    620\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Load dataset paths\n",
    "dataset_dir = \"/home/kirtimaan/Projects/archive/ORIGA/ORIGA/Images\"\n",
    "csv_file = \"/home/kirtimaan/Projects/archive/glaucoma.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Add full path to the filenames\n",
    "df['Filename'] = df['Filename'].apply(lambda x: os.path.join(dataset_dir, x))\n",
    "\n",
    "# Convert 'Glaucoma' column to string type (necessary for flow_from_dataframe)\n",
    "df['Glaucoma'] = df['Glaucoma'].astype(str)\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='Filename',\n",
    "    y_col='Glaucoma',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='Filename',\n",
    "    y_col='Glaucoma',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define Vision Transformer (ViT) model\n",
    "def create_vit_model(input_shape=(224, 224, 3), num_classes=1):\n",
    "    vit_config = ViTConfig()\n",
    "    vit = ViTModel(vit_config)\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Preprocess input for Vision Transformer (ViT)\n",
    "    vit_outputs = vit(inputs)\n",
    "    \n",
    "    # Flatten and pass through Dense layers for classification\n",
    "    x = layers.Flatten()(vit_outputs[0])  # ViT outputs a tuple, we take the hidden states\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Adding Dropout for regularization\n",
    "    x = layers.Dense(num_classes, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Create the ViT model\n",
    "vit_model = create_vit_model()\n",
    "\n",
    "# Compile the model\n",
    "vit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = vit_model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # Modify based on your needs\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = vit_model.evaluate(test_generator)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"ViT Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def patch_images(images, patch_size):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches\n",
    "\n",
    "def create_vit_model(input_shape=(224, 224, 3), patch_size=16, num_classes=1, d_model=768, num_heads=12, num_layers=12, mlp_dim=3072, dropout=0.1):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Create patches\n",
    "    patches = layers.Lambda(lambda x: patch_images(x, patch_size))(inputs)\n",
    "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
    "    \n",
    "    # Patch + position embedding\n",
    "    patch_embed = layers.Dense(d_model)(patches)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    pos_embed = layers.Embedding(input_dim=num_patches, output_dim=d_model)(positions)\n",
    "    x = patch_embed + pos_embed\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        # Multi-head attention\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x1, x1)\n",
    "        # Skip connection 1\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        # Layer normalization 2\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = layers.Dense(mlp_dim, activation=\"gelu\")(x3)\n",
    "        x3 = layers.Dense(d_model)(x3)\n",
    "        x3 = layers.Dropout(dropout)(x3)\n",
    "        # Skip connection 2\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    # Classifier head\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Create the ViT model\n",
    "vit_model = create_vit_model()\n",
    "\n",
    "# Compile the model\n",
    "vit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:105\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:514\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a `TypeSpec` for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m     element,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for 333    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n29     /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n553    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n286    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n425    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n                             ...                        \n71     /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n106    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n270    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n435    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\n102    /home/kirtimaan/Projects/archive/ORIGA/ORIGA/I...\nName: Filename, Length: 520, dtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Set up datasets\u001b[39;00m\n\u001b[1;32m     42\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 43\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m create_dataset(test_df, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate steps per epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(dataframe, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataset\u001b[39m(dataframe, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 34\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGlaucoma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(load_and_preprocess_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py:110\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 110\u001b[0m       \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;66;03m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;66;03m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/glcm/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset paths\n",
    "dataset_dir = \"/home/kirtimaan/Projects/archive/ORIGA/ORIGA/Images\"\n",
    "csv_file = \"/home/kirtimaan/Projects/archive/glaucoma.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Add full path to the filenames\n",
    "df['Filename'] = df['Filename'].apply(lambda x: f\"{dataset_dir}/{x}\")\n",
    "\n",
    "# Convert 'Glaucoma' column to integer type\n",
    "df['Glaucoma'] = df['Glaucoma'].astype(int)\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(filename, label):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "def create_dataset(dataframe, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe['Filename'], dataframe['Glaucoma']))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Set up datasets\n",
    "batch_size = 32\n",
    "train_dataset = create_dataset(train_df, batch_size)\n",
    "test_dataset = create_dataset(test_df, batch_size, shuffle=False)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "train_steps = len(train_df) // batch_size\n",
    "validation_steps = len(test_df) // batch_size\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = vit_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = vit_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = vit_model.predict(test_dataset)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Get true labels\n",
    "true_labels = test_df['Glaucoma'].values\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_classes))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predicted_classes))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "vit_model.save('vit_glaucoma_model.h5')\n",
    "print(\"Model saved as 'vit_glaucoma_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 01:02:34.517341: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-24 01:02:34.526613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 01:02:34.536727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 01:02:34.539740: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 01:02:34.547757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 01:02:35.043957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727119979.860681   51249 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-24 01:02:59.884700: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 4s/step - accuracy: 0.5598 - loss: 3.3098 - val_accuracy: 0.7404 - val_loss: 0.5732 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 4s/step - accuracy: 0.7419 - loss: 0.5909 - val_accuracy: 0.7404 - val_loss: 0.6073 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 4s/step - accuracy: 0.7419 - loss: 0.5964 - val_accuracy: 0.7404 - val_loss: 0.5846 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 4s/step - accuracy: 0.7419 - loss: 0.5915 - val_accuracy: 0.7404 - val_loss: 0.5930 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 4s/step - accuracy: 0.7419 - loss: 0.5834 - val_accuracy: 0.7404 - val_loss: 0.5801 - learning_rate: 2.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 4s/step - accuracy: 0.7419 - loss: 0.5733 - val_accuracy: 0.7404 - val_loss: 0.5803 - learning_rate: 2.0000e-04\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.7129 - loss: 0.6037\n",
      "Test accuracy: 0.7385\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        96\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.74       130\n",
      "   macro avg       0.37      0.50      0.42       130\n",
      "weighted avg       0.55      0.74      0.63       130\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[96  0]\n",
      " [34  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirtimaan/miniconda3/envs/glcm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kirtimaan/miniconda3/envs/glcm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kirtimaan/miniconda3/envs/glcm/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def patch_images(images, patch_size):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches\n",
    "\n",
    "def create_vit_model(input_shape=(224, 224, 3), patch_size=16, num_classes=1, d_model=768, num_heads=12, num_layers=12, mlp_dim=3072, dropout=0.1):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = layers.Lambda(lambda x: patch_images(x, patch_size))(inputs)\n",
    "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
    "    patch_embed = layers.Dense(d_model)(patches)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    pos_embed = layers.Embedding(input_dim=num_patches, output_dim=d_model)(positions)\n",
    "    x = patch_embed + pos_embed\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = layers.Dense(mlp_dim, activation=\"gelu\")(x3)\n",
    "        x3 = layers.Dense(d_model)(x3)\n",
    "        x3 = layers.Dropout(dropout)(x3)\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up paths\n",
    "    dataset_dir = \"/home/kirtimaan/Projects/archive/ORIGA/ORIGA/Images\"\n",
    "    csv_file = \"/home/kirtimaan/Projects/archive/glaucoma.csv\"\n",
    "\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = [os.path.join(dataset_dir, filename) for filename in df['Filename']]\n",
    "    labels = df['Glaucoma'].values\n",
    "\n",
    "    # Load all images\n",
    "    target_size = (224, 224)\n",
    "    images = np.array([load_and_preprocess_image(path, target_size) for path in image_paths])\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Create and compile the model\n",
    "    vit_model = create_vit_model(input_shape=(*target_size, 3))\n",
    "    vit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = vit_model.fit(\n",
    "        train_ds,\n",
    "        epochs=20,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = vit_model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    y_pred = vit_model.predict(test_ds)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_classes))\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected, running on CPU.\n",
      "Epoch 1/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 5s/step - accuracy: 0.6810 - loss: 3.1302 - val_accuracy: 0.7404 - val_loss: 0.6356 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 5s/step - accuracy: 0.7419 - loss: 0.6150 - val_accuracy: 0.7404 - val_loss: 0.5812 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 5s/step - accuracy: 0.7419 - loss: 0.5900 - val_accuracy: 0.7404 - val_loss: 0.5935 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 5s/step - accuracy: 0.7419 - loss: 0.5998 - val_accuracy: 0.7404 - val_loss: 0.6005 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 5s/step - accuracy: 0.7419 - loss: 0.6033 - val_accuracy: 0.7404 - val_loss: 0.6028 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 5s/step - accuracy: 0.7419 - loss: 0.5880 - val_accuracy: 0.7404 - val_loss: 0.5797 - learning_rate: 2.0000e-04\n",
      "Epoch 7/100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Ensure that TensorFlow uses the GPU if available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(f\"TensorFlow is using GPU: {physical_devices}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "def patch_images(images, patch_size):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches\n",
    "\n",
    "def create_vit_model(input_shape=(224, 224, 3), patch_size=16, num_classes=1, d_model=768, num_heads=12, num_layers=12, mlp_dim=3072, dropout=0.1):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = layers.Lambda(lambda x: patch_images(x, patch_size))(inputs)\n",
    "    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n",
    "    patch_embed = layers.Dense(d_model)(patches)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    pos_embed = layers.Embedding(input_dim=num_patches, output_dim=d_model)(positions)\n",
    "    x = patch_embed + pos_embed\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = layers.Dense(mlp_dim, activation=\"gelu\")(x3)\n",
    "        x3 = layers.Dense(d_model)(x3)\n",
    "        x3 = layers.Dropout(dropout)(x3)\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    img = Image.open(image_path).resize(target_size)\n",
    "    img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up paths\n",
    "    dataset_dir = \"/home/kirtimaan/Projects/archive/ORIGA/ORIGA/Images\"\n",
    "    csv_file = \"/home/kirtimaan/Projects/archive/glaucoma.csv\"\n",
    "\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = [os.path.join(dataset_dir, filename) for filename in df['Filename']]\n",
    "    labels = df['Glaucoma'].values\n",
    "\n",
    "    # Load all images\n",
    "    target_size = (224, 224)\n",
    "    images = np.array([load_and_preprocess_image(path, target_size) for path in image_paths])\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Create and compile the model\n",
    "    vit_model = create_vit_model(input_shape=(*target_size, 3))\n",
    "    vit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = vit_model.fit(\n",
    "        train_ds,\n",
    "        epochs=100,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = vit_model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Generate predictions on the test set\n",
    "    y_pred = vit_model.predict(test_ds)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_classes))\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: nvcc\n",
      "fish: \n",
      "nvcc --version\n",
      "^\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
